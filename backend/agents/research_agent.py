# agents/research_agent.py

from typing import Dict, Any, Optional
import json

from services.llm_client import call_gemini, google_web_search, web_fetch
from services.opik_client import create_opik_tracer

# -----------------------------------------------------------------------------
# System Prompt
# -----------------------------------------------------------------------------

RESEARCH_SYSTEM_PROMPT = """
You are an expert curriculum designer and career coach.

Given a user's goal (any domain: career, fitness, creative, etc.) and provided research content, you will:
1. Synthesize the main competencies and subskills required to achieve that goal *based on the provided research*.
2. Express them as structured JSON.
3. Avoid domain-specific jargon unless necessary; explain in plain language.
4. Ensure the competencies are grounded in the provided external content.

Output strictly valid JSON in this schema:
{
  "normalized_goal": "...",
  "competencies": [
    {
      "id": "c1",
      "name": "Short name of competency",
      "description": "1-3 sentence explanation",
      "type": "technical | conceptual | soft-skill | meta",
      "example_tasks": [
        "Example real-world task #1",
        "Example real-world task #2"
      ]
    }
  ]
}
"""

# -----------------------------------------------------------------------------
# Opik Tracer (module-level singleton)
# -----------------------------------------------------------------------------

opik_tracer = create_opik_tracer(
    name="research-agent",
    project_name="learning-paths",
    tags=["research", "competency-extraction"],
    metadata={
        "component": "research_agent",
    },
)

# -----------------------------------------------------------------------------
# Agent Execution
# -----------------------------------------------------------------------------

EVAL_SYSTEM_PROMPT = """
You are a meticulous and impartial curriculum evaluator.

Your task is to assess the quality of a list of competencies generated by another AI agent for a given user goal.

You will be given the user's goal and the generated competencies in JSON format.

Your evaluation must be based on the following criteria:
1.  **Relevance:** How relevant are the competencies to the stated goal? (e.g., Do they directly contribute to achieving the goal?)
2.  **Completeness:** Does the list cover the most critical areas needed to achieve the goal? Are there any obvious gaps?
3.  **Clarity & Actionability:** Are the competency names and descriptions clear? Are the example tasks concrete and actionable?

You must provide a score for each dimension (0-5) and a final overall score (a float between 0.0 and 1.0).

Output STRICT JSON:
{
  "dimension_scores": [
    { "name": "Relevance", "score": 0-5, "comment": "Your reasoning for the score." },
    { "name": "Completeness", "score": 0-5, "comment": "Your reasoning for the score." },
    { "name": "Clarity & Actionability", "score": 0-5, "comment": "Your reasoning for the score." }
  ],
  "overall_score": 0.0,
  "summary": "A brief summary of your evaluation."
}
"""

def eval_research_quality(goal_title: str, competencies_json: Dict[str, Any]):
    """
    Uses an LLM-as-judge to evaluate the quality of the generated competencies.
    """
    eval_user_msg = f"""
    User Goal: {goal_title}

    Generated Competencies JSON:
    {json.dumps(competencies_json, indent=2)}

    Please evaluate the quality of this output based on the criteria provided.
    """
    
    try:
        raw_output = call_gemini(
            system_instruction=EVAL_SYSTEM_PROMPT,
            user_message=eval_user_msg,
        )
        parsed_eval = json.loads(raw_output)
        return parsed_eval.get("overall_score", 0.0), parsed_eval
    except Exception:
        # In case of any failure (model call, JSON parse), return a neutral score
        return 0.5, {"error": "Evaluation failed"}


def run_research_agent(
    user_id: str,
    goal_title: str,
    goal_description: Optional[str],
    domain_hint: Optional[str],
    level: Optional[str],
) -> Dict[str, Any]:
    """
    Derives competencies for a user goal using web research.
    Fully instrumented with Opik.
    """

    # ---- Start Opik span -----------------------------------------------------
    span = None
    if opik_tracer:
        span = opik_tracer.start_span(
            name="derive_competencies",
            metadata={
                "user_id": user_id,
                "goal_title": goal_title,
                "domain_hint": domain_hint,
                "level": level,
            },
        )

    try:
        search_queries = [
            f"{goal_title} job description requirements",
            f"{goal_title} expert blogs best practices",
            f"{goal_title} github projects examples",
        ]

        # Perform web searches
        search_results = []
        for query in search_queries:
            results = google_web_search(query)
            search_results.extend(results.get("results", []))
            if span:
                span.add_event(
                    name="web_search_performed",
                    metadata={"query": query, "num_results": len(results.get("results", []))},
                )

        # Filter and get top URLs
        urls_to_fetch = []
        for res in search_results:
            if res.get("link") and res.get("link") not in urls_to_fetch:
                urls_to_fetch.append(res["link"])
            if len(urls_to_fetch) >= 5:  # Limit to top 5 URLs for content fetching
                break
        
        # Fetch content from URLs
        fetched_content = []
        if urls_to_fetch:
            for url in urls_to_fetch:
                try:
                    content = web_fetch(prompt=f"Get content from {url}")
                    fetched_content.append({"url": url, "content": content})
                    if span:
                        span.add_event(
                            name="content_fetched",
                            metadata={"url": url, "content_length": len(content)},
                        )
                except Exception as fetch_exc:
                    if span:
                        span.add_event(
                            name="content_fetch_failed",
                            metadata={"url": url, "error": str(fetch_exc)},
                        )
                    continue

        user_msg = f"""
User goal title: {goal_title}
Goal description: {goal_description or "N/A"}
Domain hint: {domain_hint or "N/A"}
Level: {level or "N/A"}

---
Research Content:
"""
        if fetched_content:
            for item in fetched_content:
                user_msg += f"\nURL: {item['url']}\nContent:\n{item['content']}\n"
        else:
            user_msg += "\nNo external research content found. Relying on general knowledge."

        user_msg += """
---

Derive competencies as described, primarily using the provided Research Content.
"""
        raw_output = call_gemini(
            system_instruction=RESEARCH_SYSTEM_PROMPT,
            user_message=user_msg,
        )

        if span:
            span.add_event(
                name="model_response_received",
                metadata={"raw_output_preview": raw_output[:500]},
            )

        try:
            parsed = json.loads(raw_output)
        except Exception as parse_error:
            parsed = {
                "normalized_goal": goal_title,
                "competencies": [],
                "error": "invalid_json_from_model",
            }

            if span:
                span.add_event(
                    name="json_parse_failure",
                    metadata={
                        "error": str(parse_error),
                        "raw_output_preview": raw_output[:500],
                    },
                )

        # ---- Evaluation hook --------------------------------------------------
        score, details = eval_research_quality(goal_title, parsed)
        if span:
            span.add_evaluation(
                name="research_quality",
                score=score,
                details=details,
            )

        return {"competencies": parsed, "research_context": fetched_content}

    except Exception as exc:
        if span:
            span.add_event(
                name="research_agent_exception",
                metadata={"error": str(exc)},
            )
        raise

    finally:
        if span:
            span.end()
